#Pan-genome analysis
roary -f ./output_directory_name -e -n -v ./*.gff
python roary_plot.py SpeciesTreeAlignment.nwk gene_presence_absence.csv


#Astral
java -jar astral.5.7.7.jar -i in.tree -o out.tre 2>out.log

#Saguaro
#before using fasta shorten headers up to 10 characters (for phylip later)
sed 's/Sample_//' GVCF_fastafile > GVCF_names_saguaro.fasta
/home/dvorikus/programs/saguaro/saguarogw-code-r44/Fasta2HMMFeature -i fasta/maf file -o file_saguaro_out -nosame
/home/dvorikus/programs/saguaro/saguarogw-code-r44/Saguaro -f binary_file_from_saguaro -o saguaro_output_dir -iter 20
ruby paint_chromosomes.rb saguaro_dir/LocalTrees.out LocalTrees.svg
Saguaro2Phylip -i saguaro.cactus
phylip neighbor infile
#put the name of the fasta file you sued to run saguaro directly in the script and then run it
ruby generate_alignments.rb LocalTrees.out /path/to/dir/with/fasta /name/of/the/output/dir/*alignment_blocks

#Calculate ANI value using fastANI
#input file is text file containing names of the genomes for comparisson (genome assemblies/.fna/fasta)
ls -1 ./dir_where_genomes_are/* > text_input_genomes.txt
fastANI --ql text_input_genomes.txt --rl text_input_genomes.txt --matrix -o fastani.out

#Gubbins 3.1.3 for recombination
conda create -n gubbins-env python=3.8
conda activate gubbins-env
conda install -c r -c defaults -c conda-forge -c bioconda gubbins
run_gubbins.py --prefix gubbins_out --filter-percentage 35.0 [default is 25, and we had lot of missing data] --first-tree-build rapidnj --first-model JC --tree-builder iqtree --model GTRGAMMA --iterations 2 [at 4 gubbins crashed] GVCFall_alignment.fasta

#run_gubbins.py --prefix gubbins_out --filter-percentage 35.0 --first-tree-build rapidnj --first-model JC (recommended as first building tree software)
--tree-builder raxmlng (iqtree crashes >3 iterations) --model GTR GVCFall_alignment.fasta (iterations default 5)
#in dir Gubbins_redo_test
#with new dataset 32 filter percentage

#view list of samples in vcf
bcftools query -l GVCF.vcf
#make text files with samples names and populations
#make subset population vcfs
bcftools view -S ./D2.txt -o GVCF_D2.vcf (your vcf)GVCF.vcf
bcftools view -S ./D3.txt ( -O z #if you want it zipped) -o GVCF_D3.vcf GVCF.vcf
#Estimate LD for chromosome NC_019693.1 on D2 and D3 subset populations
bgzip -c D3_snps.vcf > D3_snps.vcf.gz
tabix -p vcf D3_snps.vcf.gz
bcftools view D3_snps.vcf.gz --regions NC_019693.1 > D3_snps_chr.vcf.gz
#do the LD per population
#Let's do the LD using plink
#Prepare input files for plink, we need map+ped from vcftools first
#for thinning SNPs

vcftools --gzvcf SNP.vcf.gz --recode --recode-INFO-all --thin 250(for both) --out snp_chr_thin.vcf.gz

vcftools --vcf snps_chr.vcf.recode.gz --plink --out D2forLD

conda activate bedops-env
(plink v1.90b6.21)

plink --vcf D2_snps_chr.vcf.gz --keep-allele-order --indiv-sort file SampleSort.list (this is text file, that has input like fam file 0 first column, second sample names as in vcf)
--const-fid 0 (we have _ in names of samples) --allow-extra-chr 0 --make-bed --out D2forLD
#after this we have map, ped, bed, bim and fam file. Ready for LD (calculate r2 between SNPs in 50kb sw)
plink --file D2forLD --r2 --ld-window-kb 50 --ld-window 99999 --ld-window-r2 0 --out D2_snps_50kb_ld
#pairwise over whole genome
plink --file D2forLD --r2 --ld-window 999999 --ld-window-kb 9999999 --ld-window-r2 0 --out D2_pairwise_snps_ld

cat snp-thin.ld | sed 1,1d | awk -F " " 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS="\t"}{print abs($5-$2),$7}' |
sort -k1,1n > snp-thin.ld.summary
